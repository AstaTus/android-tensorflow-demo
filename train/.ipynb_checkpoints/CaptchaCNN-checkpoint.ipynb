{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('train_images_path', './datasets/images/train', 'Path to training images.')\n",
    "tf.app.flags.DEFINE_string('test_images_path', './datasets/images/test', 'Path to training images.')\n",
    "tf.app.flags.DEFINE_string('model_output_path', './datasets/output/', 'Path to model checkpoint.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_captcha_images(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise ValueError('image path is not exist')\n",
    "        \n",
    "    images = []\n",
    "    labels = []\n",
    "    images_path = os.path.join(image_path, '*.jpg')\n",
    "    count = 0\n",
    "    for image_file in glob.glob(images_path):\n",
    "        count += 1\n",
    "        if count % 5000 == 0:\n",
    "            print('Load {} images.'.format(count))\n",
    "        image = cv2.imread(image_file)\n",
    "        \n",
    "        #image = image[:, :, (2, 1, 0)] # change channel\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = int(image_file.split('_')[-1].split('.')[0])\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(inputs):\n",
    "    preprocessed_inputs = tf.to_float(inputs)\n",
    "    preprocessed_inputs = tf.subtract(preprocessed_inputs, 128.0)\n",
    "    preprocessed_inputs = tf.div(preprocessed_inputs, 128.0)\n",
    "    return preprocessed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"cnn 模型层级：\n",
    "    ·conv1\n",
    "    ·conv2\n",
    "    ·pool3\n",
    "    ·conv4\n",
    "    ·conv5\n",
    "    ·pool6\n",
    "    ·conv7\n",
    "    ·conv8\n",
    "    ·fc9\n",
    "    ·fc10\"\"\"\n",
    "def cnn(x):\n",
    "    #[n, 28, 28, 3]\n",
    "    with tf.name_scope('reshape1'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 3])\n",
    "        \n",
    "    #[n, 28, 28, 32]\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([3, 3, 3, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        layer_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    \n",
    "     #[n, 28, 28, 32]\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "        b_conv2 = bias_variable([32])\n",
    "        layer_conv2 = tf.nn.relu(conv2d(layer_conv1, W_conv2) + b_conv2)\n",
    "        \n",
    "    #[n, 14, 14, 32]\n",
    "    with tf.name_scope('pool3'):\n",
    "        layer_pool3 = max_pool_2x2(layer_conv2)\n",
    "    \n",
    "    #[n, 14, 14, 64]\n",
    "    with tf.name_scope('conv4'):\n",
    "        W_conv4 = weight_variable([3, 3, 32, 64])\n",
    "        b_conv4 = bias_variable([64])\n",
    "        layer_conv4 = tf.nn.relu(conv2d(layer_pool3, W_conv4) + b_conv4)\n",
    "    \n",
    "    #[n, 14, 14, 64]\n",
    "    with tf.name_scope('conv5'):\n",
    "        W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "        b_conv5 = bias_variable([64])\n",
    "        layer_conv5 = tf.nn.relu(conv2d(layer_conv4, W_conv5) + b_conv5) \n",
    "    \n",
    "    #[n, 7, 7, 64]\n",
    "    with tf.name_scope('pool6'):\n",
    "        layer_pool6 = max_pool_2x2(layer_conv5)\n",
    "        \n",
    "    #[n, 7, 7, 128]\n",
    "    print(layer_pool6.shape)\n",
    "    with tf.name_scope('conv7'):\n",
    "        W_conv7 = weight_variable([3, 3, 64, 128])\n",
    "        b_conv7 = bias_variable([128])\n",
    "        layer_conv7 = tf.nn.relu(conv2d(layer_pool6, W_conv7) + b_conv7)\n",
    "        \n",
    "    #[n, 7, 7, 128]\n",
    "    with tf.name_scope('conv8'):\n",
    "        W_conv8 = weight_variable([3, 3, 128, 128])\n",
    "        b_conv8 = bias_variable([128])\n",
    "        layer_conv8 = tf.nn.relu(conv2d(layer_conv7, W_conv8) + b_conv8)\n",
    "    \n",
    "    #[n, 7, 7, 128]\n",
    "    with tf.name_scope('reshape9'):\n",
    "        layer_reshape9 = tf.reshape(layer_conv8, [-1, 7 * 7 * 128])\n",
    "        \n",
    "    #[n, 7, 7, 128]\n",
    "    with tf.name_scope('fc10'):\n",
    "        W_fc10 = weight_variable([7 * 7 * 128, 1024])\n",
    "        b_fc10 = bias_variable([1024])\n",
    "        layer_fc10 = tf.nn.relu(tf.matmul(layer_reshape9, W_fc10) + b_fc10)\n",
    "        \n",
    "    with tf.name_scope('dropout11'):\n",
    "        dropout_prob = tf.placeholder(tf.float32)\n",
    "        layer_dropout11 = tf.nn.dropout(layer_fc10, dropout_prob)\n",
    "        \n",
    "    with tf.name_scope('fc12'):\n",
    "        W_fc12 = weight_variable([1024, 10])\n",
    "        b_fc12 = bias_variable([10])\n",
    "        y = tf.matmul(layer_dropout11, W_fc12) + b_fc12\n",
    "        \n",
    "    return y, dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(labels, predict_labels):\n",
    "#     cross_entropy = tf.reduce_mean(\n",
    "#         tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#             logits=predict_labels, labels=labels))\n",
    "    \n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels = labels, logits = predict_labels)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch_set(images, labels, batch_size=128):\n",
    "    \"\"\"Generate a batch training data.\n",
    "    \n",
    "    Args:\n",
    "        images: A 4-D array representing the training images.\n",
    "        labels: A 1-D array representing the classes of images.\n",
    "        batch_size: An integer.\n",
    "        \n",
    "    Return:\n",
    "        batch_images: A batch of images.\n",
    "        batch_labels: A batch of labels.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(images), batch_size)\n",
    "    batch_images = images[indices]\n",
    "    batch_labels = labels[indices]\n",
    "    return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 3], name='inputs')\n",
    "    y_ = tf.placeholder(tf.int32, shape=[None], name='labels')\n",
    "    \n",
    "    x = preprocess(x)\n",
    "    \n",
    "    predict_labels, dropout_prob = cnn(x)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=predict_labels, labels=y_))\n",
    "\n",
    "    \n",
    "    with tf.name_scope('adam_optimizer'):\n",
    "        #train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.1, global_step, 150, 0.9)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "        train_step = optimizer.minimize(cross_entropy, global_step)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        logits = tf.nn.softmax(predict_labels)\n",
    "        classes = tf.cast(tf.argmax(logits, axis=1), dtype=tf.int32)\n",
    "        classes_ = tf.identity(classes, name='classes')\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(classes, y_), 'float'))\n",
    "        \n",
    "#         correct_prediction = tf.equal(tf.argmax(predict_labels, 1), y_)\n",
    "#         correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "#         accuracy = tf.reduce_mean(correct_prediction)\n",
    "        \n",
    "    train_images, train_targets = read_captcha_images(FLAGS.train_images_path)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(1000):\n",
    "            batch_images, batch_labels = next_batch_set(train_images, train_targets)\n",
    "            train_dict = {x: batch_images, y_: batch_labels, dropout_prob: 1.0}\n",
    "            sess.run(train_step, feed_dict=train_dict)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                loss_, acc_ = sess.run([cross_entropy, accuracy], feed_dict=train_dict)\n",
    "                train_text = 'step: {}, loss: {}, acc: {}'.format(i+1, loss_, acc_)\n",
    "                print(train_text)\n",
    "#                 train_accuracy = accuracy.eval(feed_dict={\n",
    "#                     x: batch_images, y_: batch_labels, dropout_prob: 1.0})\n",
    "#                 print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "                \n",
    "            #train_step.run(feed_dict={x: batch_images, y_: batch_labels, dropout_prob: 0.5})\n",
    "\n",
    "        test_images, test_targets = read_captcha_images(FLAGS.test_images_path)\n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "            x: test_images, y_: test_targets, dropout_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 64)\n",
      "Load 5000 images.\n",
      "Load 10000 images.\n",
      "Load 15000 images.\n",
      "Load 20000 images.\n",
      "Load 25000 images.\n",
      "Load 30000 images.\n",
      "Load 35000 images.\n",
      "Load 40000 images.\n",
      "step: 1, loss: 3.218753188807743e+27, acc: 0.1171875\n",
      "step: 101, loss: nan, acc: 0.1015625\n",
      "step: 201, loss: nan, acc: 0.109375\n",
      "step: 301, loss: nan, acc: 0.1171875\n",
      "step: 401, loss: nan, acc: 0.0390625\n",
      "step: 501, loss: nan, acc: 0.0703125\n",
      "step: 601, loss: nan, acc: 0.0859375\n",
      "step: 701, loss: nan, acc: 0.078125\n",
      "step: 801, loss: nan, acc: 0.0859375\n",
      "step: 901, loss: nan, acc: 0.078125\n",
      "step: 1001, loss: nan, acc: 0.1484375\n",
      "step: 1101, loss: nan, acc: 0.1640625\n",
      "step: 1201, loss: nan, acc: 0.1015625\n",
      "step: 1301, loss: nan, acc: 0.09375\n",
      "step: 1401, loss: nan, acc: 0.140625\n",
      "step: 1501, loss: nan, acc: 0.1328125\n",
      "step: 1601, loss: nan, acc: 0.125\n",
      "step: 1701, loss: nan, acc: 0.1328125\n",
      "step: 1801, loss: nan, acc: 0.0859375\n",
      "step: 1901, loss: nan, acc: 0.0546875\n",
      "step: 2001, loss: nan, acc: 0.1015625\n",
      "step: 2101, loss: nan, acc: 0.1015625\n",
      "step: 2201, loss: nan, acc: 0.0546875\n",
      "step: 2301, loss: nan, acc: 0.0625\n",
      "step: 2401, loss: nan, acc: 0.0390625\n",
      "step: 2501, loss: nan, acc: 0.078125\n",
      "step: 2601, loss: nan, acc: 0.109375\n",
      "step: 2701, loss: nan, acc: 0.1015625\n",
      "step: 2801, loss: nan, acc: 0.0703125\n",
      "step: 2901, loss: nan, acc: 0.03125\n",
      "step: 3001, loss: nan, acc: 0.1171875\n",
      "step: 3101, loss: nan, acc: 0.09375\n",
      "step: 3201, loss: nan, acc: 0.125\n",
      "step: 3301, loss: nan, acc: 0.0859375\n",
      "step: 3401, loss: nan, acc: 0.109375\n",
      "step: 3501, loss: nan, acc: 0.09375\n",
      "step: 3601, loss: nan, acc: 0.1328125\n",
      "step: 3701, loss: nan, acc: 0.0625\n",
      "step: 3801, loss: nan, acc: 0.140625\n",
      "step: 3901, loss: nan, acc: 0.109375\n",
      "step: 4001, loss: nan, acc: 0.1015625\n",
      "step: 4101, loss: nan, acc: 0.0625\n",
      "step: 4201, loss: nan, acc: 0.1015625\n",
      "step: 4301, loss: nan, acc: 0.0859375\n",
      "step: 4401, loss: nan, acc: 0.109375\n",
      "step: 4501, loss: nan, acc: 0.1171875\n",
      "step: 4601, loss: nan, acc: 0.109375\n",
      "step: 4701, loss: nan, acc: 0.109375\n",
      "step: 4801, loss: nan, acc: 0.1328125\n",
      "step: 4901, loss: nan, acc: 0.078125\n",
      "step: 5001, loss: nan, acc: 0.0703125\n",
      "step: 5101, loss: nan, acc: 0.1171875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
